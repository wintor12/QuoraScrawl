{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "second_plain_text3_path = '/Users/tongwang/Desktop/disney/crawl2/second_plain_text3/'\n",
    "second_plain_text3_noedit_path = '/Users/tongwang/Desktop/disney/crawl2/second_plain_text3_noedit/'\n",
    "second_batch2_plain_text3_path = '/Users/tongwang/Desktop/disney/crawl2/second_batch2_plain_text3/'\n",
    "plain_text3_path = '/Users/tongwang/Desktop/disney/crawl/plain_text3/'\n",
    "batch2_plain_text3_path = '/Users/tongwang/Desktop/disney/crawl/batch2_plain_text3/'\n",
    "plain_text3_noedit_path = '/Users/tongwang/Desktop/disney/crawl/plain_text3/noedit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEditNameAndText(path):\n",
    "    texts, names = getTextAndName(path)\n",
    "    editText = []\n",
    "    editName = []\n",
    "    for i in range(len(names)):\n",
    "        if 'Edit' in texts[i]:\n",
    "            editText.append(texts[i])\n",
    "            editName.append(names[i])\n",
    "    return editText, editName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editText, editName = getEditNameAndText(second_batch2_plain_text3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(editName[3])\n",
    "print(editText[3])\n",
    "print(len(editName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in editName:\n",
    "    with open(second_plain_text3_noedit_path + name, 'r') as p:\n",
    "        st = p.read()\n",
    "        paragraphs = st.split('\\n')\n",
    "        index = 0\n",
    "        for i in range(len(paragraphs)):\n",
    "            if re.match(r'^\\s*([#([*])*Edit', paragraphs[i]):\n",
    "                index = i\n",
    "                break\n",
    "        if index > 0:\n",
    "            paragraphs = paragraphs[0:index]\n",
    "        newText = '\\n'.join(paragraphs)\n",
    "        with open(second_plain_text3_noedit_path + name, 'w') as q:\n",
    "            q.write(newText)\n",
    "            print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Copy second crawl files to stories_scrawl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = getNames(second_batch2_plain_text3_path)\n",
    "copyFiles2Folder(names, second_batch2_plain_text3_path, '/Users/tongwang/Desktop/disney/stories_crawl2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28320\n"
     ]
    }
   ],
   "source": [
    "names = getNames('/Users/tongwang/Desktop/disney/stories_crawl2/')\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/tongwang/Desktop/disney/crawl2/'\n",
    "pics = []\n",
    "views = []\n",
    "upvotes = []\n",
    "pics2 = []\n",
    "views2 = []\n",
    "upvotes2 = []\n",
    "with open(path + 'story_upvotes.txt') as p:\n",
    "    upvotes = p.read().split('\\n')\n",
    "with open(path + 'story_views.txt') as p:\n",
    "    views = p.read().split('\\n')\n",
    "with open(path + 'story_pics.txt') as p:\n",
    "    pics = p.read().split('\\n')\n",
    "with open(path + 'batch2_story_answer_urls_upvotes.txt') as p:\n",
    "    upvotes2 = p.read().split('\\n')\n",
    "with open(path + 'batch2_story_answer_urls_views.txt') as p:\n",
    "    views2 = p.read().split('\\n')\n",
    "with open(path + 'batch2_story_answer_urls_pics.txt') as p:\n",
    "    pics2 = p.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17214, 17214, 17214, 11106, 11106, 11106)\n",
      "(28320, 28320, 28320)\n"
     ]
    }
   ],
   "source": [
    "print(len(pics), len(views), len(upvotes), len(pics2), len(views2), len(upvotes2))\n",
    "upvotes += upvotes2\n",
    "pics += pics2\n",
    "views += views2\n",
    "print(len(upvotes), len(views), len(pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story_path = '/Users/tongwang/Desktop/disney/stories_crawl2/'\n",
    "names = getNames(story_path)\n",
    "texts = []\n",
    "for name in names:\n",
    "    with open(story_path + name) as p:\n",
    "        texts.append(p.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54365_Short Stories_8.txt\n",
      "Sometimes innocence of one person can result into a dangerous situation for others, HOW: Read the story ;)\n",
      "Once upon a time, when we were in school, i guess we were in 7th or 8th standard a new boy joined our class.\n",
      "We were a small class having 13 boys and 13 girls (yes perfect boy to girl ratio which never happened there after, and specially when you select Mechanical Engineering ;P)\n",
      "So usually we all boys were used to spend free time together in school and as was the tradition there was a nick name for every teacher.\n",
      "We fondly used to called one teacher as \"Gattu\" and in our day to day conversation we used to refer him by this name only.\n",
      "One fine day some other teacher wanted to call \"Gattu\" for some discussion so he told our new fellow to go and call him.\n",
      "The Newcomer goes to him and very innocently calls him - \"Gattu Sir..... Gattu sir, they are calling you\"\n",
      "To this he turns and angrily asked \"Who is Gattu... to whom you are calling\"\n",
      "(The poor fellow didn't know about the nicknames and assumed his actual name itself is Gattu)\n",
      "He got frightened and said everybody calls you Gattu only, eventually Gattu understands the matter and after that what happened is documented in the written history of our school :D\n",
      "The whole class got the deserved punishment (no further details required ;D) and the innocence of one person proved a matter of life and death for the rest of the class.\n",
      "PS : Luckily I was absent that day in school :)\n",
      "26\n",
      "1500\n",
      "0\n",
      "=========\n",
      "Sometimes innocence of one person can result into a dangerous situation for others, HOW: Read the story ;)\n",
      "Once upon a time, when we were in school, i guess we were in 7th or 8th standard a new boy joined our class.\n",
      "We were a small class having 13 boys and 13 girls (yes perfect boy to girl ratio which never happened there after, and specially when you select Mechanical Engineering ;P)\n",
      "So usually we all boys were used to spend free time together in school and as was the tradition there was a nick name for every teacher.\n",
      "We fondly used to called one teacher as \"Gattu\" and in our day to day conversation we used to refer him by this name only.\n",
      "One fine day some other teacher wanted to call \"Gattu\" for some discussion so he told our new fellow to go and call him.\n",
      "The Newcomer goes to him and very innocently calls him - \"Gattu Sir..... Gattu sir, they are calling you\"\n",
      "To this he turns and angrily asked \"Who is Gattu... to whom you are calling\"\n",
      "(The poor fellow didn't know about the nicknames and assumed his actual name itself is Gattu)\n",
      "He got frightened and said everybody calls you Gattu only, eventually Gattu understands the matter and after that what happened is documented in the written history of our school :D\n",
      "The whole class got the deserved punishment (no further details required ;D) and the innocence of one person proved a matter of life and death for the rest of the class.\n",
      "PS : Luckily I was absent that day in school :)\n",
      "26\n",
      "1500\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "html_path1 = '/Users/tongwang/Desktop/disney/crawl2/second_answer_html/'\n",
    "html_path2 = '/Users/tongwang/Desktop/disney/crawl2/second_batch2_answer_html/'\n",
    "i = 24888\n",
    "print(names[i])\n",
    "print(texts[i])\n",
    "print(upvotes[i])\n",
    "print(views[i])\n",
    "print(pics[i])\n",
    "html_path = html_path1 if i < 17214 else html_path2\n",
    "src = readHtml(html_path + names[i][:-4] + '.html')\n",
    "soup = BeautifulSoup(src)\n",
    "print('=========')\n",
    "print(getTextWithNewline(soup))\n",
    "print(getUpvotes(soup))\n",
    "print(getViews(soup))\n",
    "print(getNumPic(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28320\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(len(upvotes))\n",
    "with open('upvotes.txt', 'w') as p:\n",
    "    p.write('\\n'.join(upvotes))\n",
    "\n",
    "with open('views.txt', 'w') as p:\n",
    "    p.write('\\n'.join(views))\n",
    "\n",
    "with open('pics.txt', 'w') as p:\n",
    "    p.write('\\n'.join(pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1477_Anecdotes_8.txt\n",
      "1477_Anecdotes_8.txt\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/tongwang/Desktop/disney/story_features/titles.txt') as p:\n",
    "    titles = p.read().split('\\n')\n",
    "print titles == names\n",
    "print titles[998]\n",
    "print names[998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get inquir feature and grammar feature On server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##load texts and other features\n",
    "def getNames(path):\n",
    "    ans_names = os.listdir(path)\n",
    "    ans_names.sort(key=lambda x: int(x.split('_')[0]))\n",
    "    return ans_names\n",
    "\n",
    "\n",
    "story_path = '/home/tongwang/data/stories_crawl2/'\n",
    "names = getNames(story_path)\n",
    "texts = []\n",
    "for name in names:\n",
    "    with open(story_path + name) as p:\n",
    "        texts.append(p.read())\n",
    "        \n",
    "with open('/home/tongwang/data/story_crawl2_features/upvotes.txt') as p:\n",
    "    upvotes = p.read().splitlines()\n",
    "\n",
    "with open('/home/tongwang/data/story_crawl2_features/views.txt') as p:\n",
    "    views = p.read().splitlines()\n",
    "\n",
    "with open('/home/tongwang/data/story_crawl2_features/pics.txt') as p:\n",
    "    pics = p.read().splitlines()\n",
    "\n",
    "print(len(upvotes), len(views), len(pics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##load inquirer features\n",
    "def find(header_name):\n",
    "    dataIn = pd.read_csv(\"/home/tongwang/data/inquirerbasic.csv\",header = 0)\n",
    "    column_content = dataIn[header_name]\n",
    "    entries = dataIn['Entry']\n",
    "    output_entries = []\n",
    "    if (len(entries) != len(column_content)):\n",
    "        print(\"not same length\")\n",
    "    \n",
    "    for i in range(0,len(entries)):\n",
    "        if column_content[i] == header_name:\n",
    "            if \"#\" not in entries[i] or \"#1\" in entries[i]:\n",
    "                output_entries.append(entries[i].strip( '#1' ))\n",
    "    \n",
    "    return [x.lower() for x in output_entries]\n",
    "\n",
    "inquirer = {}\n",
    "category=['Positiv', 'Negativ', 'Strong', 'Weak', 'Active', 'Passive', \n",
    "          'Pleasur', 'Pain', 'Feel', 'Arousal', 'EMOT', 'Virtue', 'Vice', 'Ovrst', 'Undrst',\n",
    "          'Role', 'COLL', 'Work', 'Ritual', 'SocRel', 'Object']\n",
    "\n",
    "inquirer_voc = []\n",
    "for c in category:\n",
    "    words = find(c)\n",
    "    inquirer[c] = words\n",
    "    inquirer_voc += words\n",
    "\n",
    "inquirer_voc = set(inquirer_voc)\n",
    "text_tokens = [nltk.word_tokenize(re.sub(r'[^\\x00-\\x7F]+',' ', x.lower())) for x in texts]\n",
    "\n",
    "pattern = '^[a-z]+$'\n",
    "inquir_count = np.zeros((len(names), 21))\n",
    "for i in range(len(names)):\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    tokens = text_tokens[i]\n",
    "    for t in tokens:\n",
    "        if re.match(pattern, t) != None and t in inquirer_voc:\n",
    "            for j in range(len(category)):\n",
    "                if t in inquirer[category[j]]:\n",
    "                    inquir_count[i][j] += 1\n",
    "\n",
    "inquir_normcount = np.zeros((len(names), 21))\n",
    "for i in range(len(names)):\n",
    "    if len(text_tokens[i])!= 0:\n",
    "        inquir_normcount[i] = inquir_count[i]/len(text_tokens[i])\n",
    "\n",
    "\n",
    "np.savetxt('/home/tongwang/data/story_crawl2_features/inquirer.txt', inquir_normcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import language_check\n",
    "def computeGrammarErrors(X_train):\n",
    "    tool = language_check.LanguageTool('en-US')\n",
    "    g_train=[]\n",
    "    for text in X_train:\n",
    "        matches = tool.check(text.decode('utf-8'))\n",
    "        g_train.append(len(matches))\n",
    "    \n",
    "    return g_train\n",
    "\n",
    "grammars = computeGrammarErrors(texts)\n",
    "np.savetxt('/home/tongwang/data/story_crawl2_features/grammars.txt', grammars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put all features together to one file\n",
    "with open('/home/tongwang/data/story_crawl2_features/grammars.txt') as p:\n",
    "    grammars = p.read().splitlines()\n",
    "    \n",
    "upvotes = np.array(upvotes).astype(float)\n",
    "pics = np.array(pics).astype(float)\n",
    "views = np.array(views).astype(float)\n",
    "grammars = np.array(grammars).astype(float)\n",
    "wordcounts = [len(t) for t in text_tokens]\n",
    "wordcounts = np.array(wordcounts).astype(float)\n",
    "feature_all = [views, pics, wordcounts]\n",
    "feature_all = np.transpose(np.asarray(feature_all))\n",
    "feature_all = np.append(feature_all, inquir_normcount, axis = 1)\n",
    "feature_all = np.append(feature_all, grammars.reshape(-1,1), axis = 1)\n",
    "np.savetxt('/home/tongwang/data/story_crawl2_features/features.txt', feature_all)\n",
    "np.savetxt('/home/tongwang/data/story_crawl2_features/labels.txt', upvotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare crawl1 and crawl2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_all1 = np.loadtxt('/home/tongwang/data/story_features/features.txt')\n",
    "wc = feature_all[:,2]\n",
    "wc1 = feature_all_1[:,2]\n",
    "v = feature_all[:,0]\n",
    "v1 = feature_all_1[:,0]\n",
    "#count word count < 50\n",
    "print(wc1[wc1<50].shape)\n",
    "print(wc[wc<50].shape)\n",
    "\n",
    "\n",
    "#count view < 50\n",
    "print(v1[v1<50].shape)\n",
    "print(v[v<50].shape)\n",
    "\n",
    "#count difference between views\n",
    "diff = v >= v1\n",
    "print(diff[diff==True].shape)\n",
    "\n",
    "#compare upvotes\n",
    "upvotes1 = np.loadtxt('/home/tongwang/data/story_features/labels.txt')\n",
    "diff2 = upvotes >= upvotes1\n",
    "print(diff2[diff2==True].shape)\n",
    "\n",
    "#compare texts\n",
    "story_path1 = '/home/tongwang/data/stories/'\n",
    "names1 = getNames(story_path1)\n",
    "texts1 = []\n",
    "for name in names1:\n",
    "    with open(story_path1 + name) as p:\n",
    "        texts1.append(p.read())\n",
    "\n",
    "count = 0\n",
    "for i in range(28320):\n",
    "    if texts[i] != texts1[i]:\n",
    "        count +=1\n",
    "\n",
    "print count\n",
    "#count answer deleted\n",
    "print(len([t for t in texts if t == 'Answer deleted']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split crawl2 dataset to train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get invalid titles\n",
    "with open('/home/tongwang/data/story_features/titles.txt') as p:\n",
    "    titles1 = p.read().splitlines()\n",
    "\n",
    "print(len(titles1))\n",
    "with open('/home/tongwang/data/story_crawl2_features/titles.txt') as p:\n",
    "    titles = p.read().splitlines()\n",
    "\n",
    "print(len(titles))\n",
    "print(titles == titles1)\n",
    "\n",
    "#get indice of wordcount < 50\n",
    "indice_wc = np.argwhere(wc<50).flatten()\n",
    "#get indice of view < 50\n",
    "indice_v = np.argwhere(v<50).flatten()\n",
    "\n",
    "indice = np.unique(np.concatenate((indice_wc, indice_v)))\n",
    "#get invalid titles\n",
    "ititles = [titles[x] for x in indice]\n",
    "\n",
    "with open('/home/tongwang/data/story_crawl2_features/invalid_titles.txt', 'w') as p:\n",
    "    p.write('\\n'.join(ititles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTitles(path):\n",
    "    with open(path+'title_train.txt') as p:\n",
    "        t_train = p.read()\n",
    "    with open(path+'title_val.txt') as p:\n",
    "        t_val = p.read()\n",
    "    with open(path+'title_test.txt') as p:\n",
    "        t_test = p.read()\n",
    "    return t_train.split('\\n'),t_val.split('\\n'), t_test.split('\\n')\n",
    "\n",
    "path = '/home/tongwang/data/story_dataset_newsplit/'\n",
    "titles_train, titles_val, titles_test = loadTitles(path)\n",
    "print(len(titles_train), len(titles_val), len(titles_test))\n",
    "\n",
    "titles_train = [t for t in titles_train if t not in ititles]\n",
    "titles_val = [t for t in titles_val if t not in ititles]\n",
    "titles_test = [t for t in titles_test if t not in ititles]\n",
    "print(len(titles_train), len(titles_val), len(titles_test))\n",
    "\n",
    "indice_train = [titles.index(t) for t in titles_train]\n",
    "indice_val = [titles.index(t) for t in titles_val]\n",
    "indice_test = [titles.index(t) for t in titles_test]\n",
    "print(len(indice_train), len(indice_val), len(indice_test))\n",
    "\n",
    "feature_train = feature_all[indice_train]\n",
    "feature_val = feature_all[indice_val]\n",
    "feature_test = feature_all[indice_test]\n",
    "\n",
    "label_train = upvotes[indice_train]\n",
    "label_val = upvotes[indice_val]\n",
    "label_test = upvotes[indice_test]\n",
    "\n",
    "question_train = [questions[i] for i in indice_train]\n",
    "question_val = [questions[i] for i in indice_val]\n",
    "question_test = [questions[i] for i in indice_test]\n",
    "\n",
    "def copyFiles2Folder(names, spath, dpath):\n",
    "    for name in names:\n",
    "        shutil.copyfile(spath + name, dpath + name)\n",
    "\n",
    "story_path = '/home/tongwang/data/stories_crawl2/'\n",
    "copyFiles2Folder(titles_train, story_path, '/home/tongwang/data/story_dataset_newsplit_scrawl2/train/')\n",
    "copyFiles2Folder(titles_val, story_path, '/home/tongwang/data/story_dataset_newsplit_scrawl2/val/')\n",
    "copyFiles2Folder(titles_test, story_path, '/home/tongwang/data/story_dataset_newsplit_scrawl2/test/')\n",
    "\n",
    "new_path = '/home/tongwang/data/story_dataset_newsplit_scrawl2/'\n",
    "np.savetxt(new_path + 'feature_train', feature_train)\n",
    "np.savetxt(new_path + 'feature_val', feature_val)\n",
    "np.savetxt(new_path + 'feature_test', feature_test)\n",
    "\n",
    "np.savetxt(new_path + 'y_train', label_train)\n",
    "np.savetxt(new_path + 'y_val', label_val)\n",
    "np.savetxt(new_path + 'y_test', label_test)\n",
    "\n",
    "with open(new_path + 'question_train.txt', 'w') as p:\n",
    "    p.write('\\n'.join(question_train))\n",
    "\n",
    "with open(new_path + 'question_val.txt', 'w') as p:\n",
    "    p.write('\\n'.join(question_val))\n",
    "\n",
    "with open(new_path + 'question_test.txt', 'w') as p:\n",
    "    p.write('\\n'.join(question_test))\n",
    "\n",
    "with open(new_path + 'title_train.txt', 'w') as p:\n",
    "    p.write('\\n'.join(titles_train))\n",
    "\n",
    "with open(new_path + 'title_val.txt', 'w') as p:\n",
    "    p.write('\\n'.join(titles_val))\n",
    "\n",
    "with open(new_path + 'title_test.txt', 'w') as p:\n",
    "    p.write('\\n'.join(titles_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
